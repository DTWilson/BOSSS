---
title: "BOSSS"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{BOSSS}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = TRUE
)
```

```{r setup}
library(BOSSS)
library(ggplot2)
library(mco)
```

<!-- Note - a Twitter thread on power analysis had a couple of people mentioning the R package simr for power through simulation when using mixed models. So might be worth including that in one of our examples, to save people having to write their own simulation code? -->

Every application of BOSSS follows the same set of steps:

1. Formally specify the problem and create a corresponding `BOSSS_problem` object;
2. Create a `BOSSS_solution` object and initialise it;
3. Perform a number of iterations, each one updating and (hopefully) improving the `BOSSS_solution` object;
4. Select a final design and run some diagnostics to check it is valid.

We will work through these steps here for an example problem: designing a cluster randomised trial, as discussed in Section 2 of the manuscript associated with the package. For more illustrations, see the \code{vignette("Examples", package = "BOSSS")} article.

### Problem

The first ingredient of a BOSSS problem is the simulation function. The arguments of this function must first specify the _design variables_  which we want to vary in our problem, followed by the _model parameters_. In this example we have two design variables representing the number of clusters in each arm, and the number of participants in each arm. The model parameters are the mean difference in outcome between the control and experimental arms (`beta_1`), the within-cluster variance (`var_e`), and the between-cluster variance (`var_u`). Note that all of these inputs require some defaults to be provided.

The other element of interface the simulation function must conform to is in its return value, which should be a named vector of the quantities who's mean values we want to estimate using the Monte Carlo method. Here, we have one such quantity: a boolean indicator of a negative decision, `s`.

```{r}
sim_cRCT <- function(m = 10, n = 20, 
                     beta_1 = 0.3, var_e = 0.95, var_u = 0.05){
  
  m <- floor(m); n <- floor(n)
  
  x <- rep(c(0,1), each = m) 
  y <- rnorm(2*m, 0, sqrt(var_u + var_e/n))
  y <- y + x*beta_1

  s <- t.test(y[x==1], y[x==0], alternative = "greater")$p.value > 0.025

  return(c(s = s))
}

# For example,
sim_cRCT()
```

In our problem we are not just interested in the expected value of `s`; we also want to minimise the number of clusters and the number of participants. Since these are fixed quantities given any design, we evaluate them in a separate deterministic function. This should conform to the same principles as the simulation function, with the same inputs, but allowing for different named outputs. Here, the outputs are just `m`, and `N = n*m`:

```{r}
det_cRCT <- function(m = 10, n = 5, 
                     beta_1 = 0.3, var_e = 0.95, var_u = 0.05)
{
  return(c(m = floor(m), N = floor(n)*floor(m)))
}  
```

Next, we need to note the ranges of the design variables which we plan to search over. We use the `design_space()` function for this, specifying the lower and upper limits of the design variables in the order they appear as simulation function arguments:

```{r}
design_space <- design_space(lower = c(10, 5), 
                             upper = c(50, 20),
                             sim = sim_cRCT)

design_space
```

Note that the function automatically retrieves the names of the design variables based on the order they take in the simulation function.

We also need to specify the hypotheses which we're planning to simulate under, using the `hypotheses()` function, again specifying in the order that parameters appear as simulation function argument. We only need one hypothesis here, the alternative, since we will be estimating the type II error rate.

```{r}
hypotheses <- hypotheses(values = matrix(c(0.3, 0.95, 0.05), ncol = 1),
                         hyp_names = c("alt"),
                         sim = sim_cRCT)

hypotheses
```

Constraints should be specified using the `constraints` function. Each constraint should be named, and should be defined with respect to a specific output and a specific hypothesis. It should have a nominal maximum value, and a probability `delta` used to judge if it is satisfied. Here, our constraint is that the mean of the simulation output (i.e. the probability of a negative result) under the `alt` hypothesis should be less than or equal to 0.2 with a probability of at least 0.95. Finally, we note whether the constraint output is binary or otherwise.

```{r}
constraints <- constraints(name = c("con_tII"),
                   out = c("s"),
                   hyp = c("alt"),
                   nom = c(0.2),
                   delta = c(0.95),
                   binary = c(TRUE))

constraints
```

The final ingredient of the problem is the set of objectives we want to minimise. Similar to constraints, objectives are tied to a specific output and hypothesis. We also specify weights for each objective which help guide the internal optimisation process, and note whether or not the output for each objective is binary or continuous. For example, here we want to minimise both the number of patients `N` and the number of clusters `m`, with the latter carrying a weight of 100 times that of the former.

```{r}
objectives <- objectives(name = c("N", "m"),
                 out = c("N", "m"),
                 hyp = c("alt", "alt"),
                 weight = c(1, 10),
                 binary = c(FALSE, FALSE))

objectives
```

We now put this simulation function and set of data frames together to create an object of class `BOSSS_problem`. 

```{r}
prob <- BOSSS_problem(sim_cRCT, design_space, hypotheses, objectives, constraints, det_func = det_cRCT)
```

### Initialisation

Having set up the problem, we now need to create an initial solution to it. This involves setting up a space-filling set of designs spanning the design space (where `size` is the number of designs), computing the Monte Carlo estimates of all the expectations we are interested in at each of these designs (using `N` samples for each evaluation), fitting Gaussian Process models to these estimates, and then using those models to estimate the Pareto set and front:

```{r}
set.seed(9823579)

size <- 20
N <- 100

sol <- BOSSS_solution(size, N, prob)

print(sol) 
plot(sol)
```

The `print()` function will give a table of the Pareto set with associated objective function values. The `plot()` function will plot the Pareto front.

### Iteration

We can now start improving this solution by calling the `iterate()` function. Each call uses the fitted Gaussian Process models to decide on the next design to be evaluated, computes the Monte Carlo estimates at that point, and then updates the estimated Pareto set and front. 

```{r}
N <- 100
for(i in 1:20) {
  sol <- iterate(sol, prob, N) 
}
print(sol)
plot(sol)
```

We can compare the initial and final approximations to the Pareto front to the actual front since, for this simple example, we can calculate power analytically:

```{r}
df <- expand.grid(m = 10:50,
                  n = 5:20)
df$N <- df$m*df$n

df$clust_var <- 0.05 + 0.95/df$n

df$pow <- power.t.test(df$m, delta = 0.3, sd = sqrt(df$clust_var),
                       alternative = "one.sided", sig.level = 0.025)$power

df <- df[df$pow >= 0.8, c("m", "N")]
df <-  data.frame(paretoFilter(as.matrix(df)))
df$t <- "True"

df <- rbind(df, data.frame(m = c(40, 23,
                                 23, 31, 26, 20, 24, 27, 35, 21, 29),
                           N = c(320, 345,
                                 345, 248, 286, 380, 312, 270, 245, 357, 261),
                           t = c(rep("Initial", 2), rep("Final", 9))))

# Add ends of steps
df2  <- rbind(data.frame(m=c(43, 43),
                          N=c(320, 245),
                          t=c("Initial", "Final")),
              df,
              data.frame(m=c(23, 20, 19),
                         N=c(380, 380, 380),
                         t=c("Initial", "Final", "True")))

ggplot(df, aes(N, m, colour = t)) + geom_point() +
  xlab("Total sample size per arm, N") +
  ylab("Number of clusters per arm, m") +
  scale_colour_manual(name = "", values = c("#EF476F", "#FFD166", "#06D6A0")) +
  geom_step(data=df2, alpha = 0.5) +
  theme_minimal()

#ggsave(here("man", "figures", "cRCT_fronts.pdf"), height=9, width=14, units="cm")
```

### Diagnostics

To check the GP models are giving sensible predictions, we can choose a specific design and then plot the predictions for each model along the range of each design variable.

```{r}
# Pick a specific design from the Pareto set
design <- sol$p_set[1,]

ps <- diag_plots(design, prob, sol)

ps[[1]]
#ggsave(here("man", "figures", "cRCT_diag_m.pdf"), height=8, width=8, units="cm")

ps[[2]]
#ggsave(here("man", "figures", "cRCT_diag_n.pdf"), height=8, width=8, units="cm")
```


We can also get the predicted values and 95% credible intervals for each point we have evaluated, contrasting these with the empirical MC estimate and interval. This will return a data frame for each of the models, named according to the output-hypothesis combination which defines it. We highlight with a `*` any points where the two intervals do not overlap.

```{r}
dp <- diag_predictions(prob, sol)
dp[[1]][dp[[1]]$no_overlap == "*",]
```

In this example we see that the empirical estimates agree with the predictions for all but three of the designs which have been evaluated. 

At any point, but especially after initialisation, we may decide that we need more points in our initial evaluations, or more simulations at each of those points. We can do that via `extend_initial()`. For example, suppose we want to add an extra 500 simulations to our initial points and to add another ten points to the initial space-filling design:

```{r}
sol <- extend_initial(prob, sol, extra_N = 5000)

# Look at the first three designs; the empirical and predicted estimates will have changed
diag_predictions(prob, sol)[[1]][1:3,]

print(sol)
```

Or, we could add another 10 points:

```{r}
sol <- extend_initial(prob, sol, extra_points = 10)

# Look at the added designs, which go to the top of the table
diag_predictions(prob, sol)[[1]][1:10,]
```


Once we are happy with our solution and have chosen a specific design from the Pareto set, we might want to double check that point by running a large number of simulations at it.

```{r}
design <- sol$p_set[nrow(sol$p_set),]

r <- diag_check_point(design, prob, sol, N=10^6) 
```

If we decide we want to run more simulations at the same point, we can pass the previous results in so they are built upon.

```{r}
r <- diag_check_point(design, prob, sol, N=10^4, r) 
```




