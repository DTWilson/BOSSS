---
title: "BOSSS"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{BOSSS}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

```{r setup}
library(BOSSS)
library(ggplot2)
```

<!-- A place to build up notes and examples for a future tutorial-style document. -->

<!-- Note - a Twitter thread on power analysis had a couple of people mentioning the R package simr for power through simulation when using mixed models. So might be worth including that in one of our examples, to save people having to write their own simulation code? -->

Every application of BOSSS follows the same set of steps:

1. Formally specify the problem and create a corresponding `BOSSS_problem` object;
2. Create a `BOSSS_solution` object and initialise it;
3. Perform a number of iterations, each one updating and (hopefully) improving the `BOSSS_solution` object;
4. Select a final design and run some diagnostics to check it is valid.

## Adaptive design (NIFTy)

### Problem

The first ingredient of a BOSSS problem is the simulation function. This function must have two named arguments. The first, `design`, is a vector containing the values of all _design variables_ which we want to vary in our problem. In this example we have 4 design variables representing the total sample size, the fraction of the sample size used in the first stage, the $\alpha$ level used in the interim analysis, and the $\alpha$ level used in the final analysis. Note how these variables are extracted from the `design` vector.

The second argument, `hypothesis` is a vector containing the values of all _model parameters_ which we will vary in our problem. In this case this vector has four entries corresponding to the probability of the interim outcome in the control and experimental arms, and the probability of the final outcome in the control and experimental arms. Again, we extract these variables from the `hypothesis` vector at the start of the function.

The other element of interface the simulation function must conform to is in its return value, which should be a vector of the quantities who's mean values we want to estimate using the Monte Carlo method. Here, we have three such quantities: a boolean indicator of an overall positive decision (`g`), a boolean indicator of a negative decision (`s`), and the final sample size (`n`). The elements of this returned vector output should all be named.

```{r}
#n is the total sample size
#ninterim is the number of patients at the interim analysis (proportion)
#ainterim is alpha at interim analysis (threshold p-value at interim analysis)
#afinal is alpha at final analaysis (threshold p-value for 2nd and final analysis)
#this means overall alpha for the trial is ainterim*afinal
#
#pcontshort is the probability of 1 day PoSH in the control arm
#pexpshort is the probability of 1 day PoSH in the experimental arm
#pcontlong is the probability of 6 month PoSH in the control arm
#pexplong is the probability of 6 month PoSH in the experimental arm
#p01_relative is s.t. p01_relative*pexplong = probability of having 
# a long term outcome after no short term outcome

#sim_trial <- function(design = c(300, 0.5, 0.4, 0.1),
#                      hypothesis = c(0.25, 0.125, 0.1, 0.03, 0))
sim_trial <- function(n = 300, ninterim = 0.5, ainterim = 0.4, afinal = 0.1,
                      pcontshort = 0.25, pexpshort = 0.125,
                      pcontlong = 0.1, pexplong = 0.03, p01_relative = 0)
{
  ninterim <- floor(ninterim*n)
  
  patients<-c(1:n) #create patients
  
  treat<- rep(c(1,2), ceiling(n/2))[1:n]
  
  n_cont <- sum(treat == 1)
  
  short<-rep(0,n) #short term outcome
  long<-rep(0,n) #long term outcome
  
  data<-data.frame(patients,treat,short,long) #combine into dataset
  
  #generate result 1/0 for short term outcome. If short term outcome=0 then long term outcome=0
  #If short term outcome=1 then long term outcome has probability pcontlong/pcontshort
  #repeat for treatment=2
  # for(i in 1:n){
  #   if(treat[i]==1){
  #     data$short[i]<-rbinom(1,1,pcontshort)
  #     if(data$short[i]==0){
  #       data$long[i]<-0
  #     }
  #     if(data$short[i]==1){
  #       data$long[i]<-rbinom(1,1,pcontlong/pcontshort)
  #     }
  #   }
  #   else{
  #     data$short[i]<-rbinom(1,1,pexpshort)
  #     if(data$short[i]==0){
  #       data$long[i]<-0
  #     }
  #     if(data$short[i]==1){
  #       data$long[i]<-rbinom(1,1,pexplong/pexpshort)
  #     }
  #   }
  # }
  
  # An alternative and more general parametersation of the above model
  # to allow different values of probability of a long outcome after
  # no short outcome (previously hard coded as 0)

  # probability vector p00, p01, p10, p11 in (short, long) form
  # Control arm:
  p01 <- p01_relative*pcontlong
  p11 <- pcontlong - p01
  p10 <- pcontshort - p11
  # Simulate outcomes in multinomial format
  cont_count <- rmultinom(1, n_cont, c((1 - p01 - p10 - p11), p01, p10, p11))
  # Translate these to short and long outcomes
  cont_out <- matrix(c(rep(c(0,0), cont_count[1]),
                       rep(c(0,1), cont_count[2]),
                       rep(c(1,0), cont_count[3]),
                       rep(c(1,1), cont_count[4])), ncol = 2, byrow = TRUE)
  # shuffle the list randomly
  cont_out <- cont_out[sample(1:nrow(cont_out)),]
  
  # Experimental arm:
    # Control arm:
  p01 <- p01_relative*pexplong
  p11 <- pexplong - p01
  p10 <- pexpshort - p11
  exp_count <- rmultinom(1, n - n_cont, c((1 - p01 - p10 - p11), p01, p10, p11))
  exp_out <- matrix(c(rep(c(0,0), exp_count[1]),
                       rep(c(0,1), exp_count[2]),
                       rep(c(1,0), exp_count[3]),
                       rep(c(1,1), exp_count[4])), ncol = 2, byrow = TRUE)
  exp_out <- exp_out[sample(1:nrow(exp_out)),]
  
  data[data$treat == 1, c("short", "long")] <- cont_out
  data[data$treat == 2, c("short", "long")] <- exp_out
  
  #perform chi squared test on short term outcome for 1st ninterim patients
  data2<-data[1:ninterim,]
  tbl<-table(data2$short,data2$treat)
  test<- suppressWarnings(chisq.test(tbl)$p.value) #get p-value
  
  #if p<ainterim, perform chi squared test on long term outcome for all patients
  if(test<ainterim){
    tbl2<-table(data$long,data$treat)
    test2<- suppressWarnings(chisq.test(tbl2)$p.value)
  }
  
  #if p>ainterim, trial unsuccessful at interim and final analysis
  #if p<ainterim, trial successful at interim:
  #if p2>afinal, trial unsuccessful at final analysis
  #if p2<afinal, trial successful at final analysis
  if(test>=ainterim){
    return(c(g = 0, s = 1, n = ninterim))
  }
  if(test<ainterim){
    if(mean(data2[data2$treat == 1,"short"]) < mean(data2[data2$treat == 2,"short"])){
      return(c(g = 0, s = 1, n = ninterim))
    } else {
      if(test2>afinal){
        return(c(g = 0, s = 1, n = n))
      }
      if(test2<afinal){
        return(c(g = 1, s = 0, n = n))
      }
    }
  }
}

# For example,
sim_trial()
```

Next, we need to note the ranges of the design variables which we plan to search over. We use the `design_space()` function for this, specifying the lower and uppwer limits of the design variables in the order they appear as simulation function arguments:

```{r}
design_space <- design_space(lower = c(300,0.05,0,0), 
                             upper = c(700,0.5,1,1),
                             sim = sim_trial)
```

We also need to specify the hypotheses which we're planning to simulate under, using the `hypotheses()` function, again specifying in the order that parameters appear as simulation function argument. We need two hypotheses, corresponding to a null and an alternative:

```{r}
hypotheses <- hypotheses(values = matrix(c(0.25, 0.25, 0.1, 0.1, 0, 
                                           0.25, 0.125, 0.1, 0.03, 0), ncol = 2),
                         hyp_names = c("null", "alt"),
                         sim = sim_trial)
```

Note that for both the design space and the hypotheses, the names are just for reference and do not need to correspond exactly to the variables names used in the simulation function.

Constraints should be specified using the `constraints` function. Each constraint should be named, and should be defined with respect to a specific simulation output and a specific hypothesis. It should have a nominal maximum value, and a probability `delta` used to judge if it is satisfied. Finally, it should note if the function is stochastic, or deterministic. Here, our fist constraint is that the mean of the first simulation output (i.e. the probability of a positive result) under the first hypothesis (the null) should be less than 0.2 with a probability of at least 0.95.

```{r}
constraints <- constraints(name = c("a", "b"),
                   out = c("g", "s"),
                   hyp = c("null", "alt"),
                   nom = c(0.1, 0.2),
                   delta =c(0.95, 0.95),
                   stoch = c(TRUE, TRUE))
```

The final ingredient of the problem is the set of objectives we want to minimise. Similar to constraints, objectives are tied to a specific simulation output and hypothesis, and can be stochastic or deterministic. We also specify weights for each objective which help guide the optimisation process, and note whether or not the output for each objective is binary or continuous. For example, here we want to minimise both the type I and type II error rates and the expected sample size, with the former carrying a weight of 100 times of the latter.

```{r}
objectives <- objectives(name = c("f1", "f2", "f3"),
                 out = c("g", "s", "n"),
                 hyp = c("null", "alt", "null"),
                 weight = c(100, 100, 1),
                 stoch = c(TRUE, TRUE, TRUE),
                 binary = c(TRUE, TRUE, FALSE))
```

We now put this simulation function and set of data frames together to create an object of class `BOSSS_problem`. 

```{r}
prob <- BOSSS_problem(sim_trial, design_space, hypotheses, constraints, objectives)
```

### Initialisation

Having set up the problem, we now need to create an initial solution to it. This involves setting up a space-filling set of designs spanning the design space (where `size` is the number of designs), computing the Monte Carlo estimates of all the expectations we are interested in at each of these designs (using `N` samples for each evaluation), fitting GP models to these estimates, and then using those models to estimate the Pareto set and front:

```{r}
size <- 40
N <- 5000

sol <- BOSSS_solution(size, N, prob)

print(sol)
plot(sol)
```

The `print()` function will give a table of the Pareto set with associated objective function values. The `plot()` function will plot the Pareto front.

### Iteration

We can now start improving this solution by calling the `iterate()` function. Each call uses the fitted Gaussian Process models to decide on the next design to be evaluated, computes the Monte Carlo estimates at that point, and then updates the estimated Pareto set and front. 

```{r}
N <- 10000
for(i in 1:10) {
  sol <- iterate(sol, prob, N) 
}
print(sol)
plot(sol)
```

### Diagnostics

To check the GP models are giving sensible predictions, we can choose a specific design and then plot the predictions for each model along the range of each design variable.

```{r}
# Pick a specific design from the Pareto set
design <- sol$p_set[1,]

plots <- one_d_plots(design, prob, sol)

for(i in 1:length(plots)) {
  print(plots[[i]])
}
```

When we've chosen our design, we might want to check its properties by computing more MC estimates, which we can also compare against the model predictions. 

```{r}
r <- check_point(design, prob, sol, N=10^2) 
```

If we decide we want to run more simulations at the same point, we can pass the previous results in so they are built upon.

```{r}
r <- check_point(design, prob, sol, N=10^3, r) 
```
To do - have these extra checks incorporated back into the solution so they can be used if we start iterating again.

### Sensitivity analysis

Use a generalised hypothesis defined by ranges (which may be degenerate point values) for each parameter. Take a suggested design and fit new models to cover these ranges.

```{r}
sa_hypothesis <- data.frame(hyp = "null",
                            sa_param = 5,
                            lower = 0,
                            upper = 1)

sa <- as.data.frame(sensitivity(sa_hypothesis, design, prob, sol, num_eval = 20, N = 1000))

# Plot these to examine sensitivity:
ggplot(sa, aes(p01relative, g_m)) + geom_point() + 
  geom_errorbar(aes(ymin = g_m - 1.96*sqrt(g_v), ymax = g_m + 1.96*sqrt(g_v))) +
  theme_minimal()

ggplot(sa, aes(p01relative, n_m)) + geom_point() + 
  geom_errorbar(aes(ymin = n_m - 1.96*sqrt(n_v), ymax = n_m + 1.96*sqrt(n_v))) +
  theme_minimal()
```

From the above we can conclude the type I error rate and the expected sample size under the null are not sensitive to the nuisance parameter. Repeating under the alternative:

```{r}
sa_hypothesis <- data.frame(hyp = "alt",
                            sa_param = 5,
                            lower = 0,
                            upper = 1)

sa <- as.data.frame(sensitivity(sa_hypothesis, design, prob, sol, num_eval = 20, N = 1000))

# Plot these to examine sensitivity:
ggplot(sa, aes(p01relative, s_m)) + geom_point() + 
  geom_errorbar(aes(ymin = s_m - 1.96*sqrt(s_v), ymax = s_m + 1.96*sqrt(s_v))) +
  theme_minimal()

ggplot(sa, aes(p01relative, n_m)) + geom_point() + 
  geom_errorbar(aes(ymin = n_m - 1.96*sqrt(n_v), ymax = n_m + 1.96*sqrt(n_v))) +
  theme_minimal()
```

Again, no clear trend.

## NIFTY variation

Using the same simulation, we can change the problem slightly by now only constraining the type I error rate (and not treating its minimisation as an objective), and only minimising the power (not constraining it) and expected sample size:

```{r}
design_space <- design_space(name = c("n", "ninterim", "ainterim", "afinal"),
                             lower = c(300,0.05,0,0), 
                             upper = c(700,0.5,1,1))

hypotheses <- hypotheses(par_name = c("pcontshort", "pexpshort", "pcontlong", "pexplong", "p01relative"),
                         values = matrix(c(0.25, 0.25, 0.1, 0.1, 0, 
                                                 0.25, 0.125, 0.1, 0.03, 0), ncol = 2),
                         hyp_names = c("null", "alt"))

constraints <- constraints(name = c("a", "b"),
                   out = c("g"),
                   hyp = c("null"),
                   nom = c(0.1),
                   delta =c(0.95),
                   stoch = c(TRUE))

objectives <- objectives(name = c("f1", "f2"),
                 out = c("s", "n"),
                 hyp = c("alt", "null"),
                 weight = c(100, 1),
                 stoch = c(TRUE, TRUE),
                 binary = c(TRUE, FALSE))

prob <- BOSSS_problem(sim_trial, design_space, hypotheses, constraints, objectives)

size <- 40
N <- 500

sol <- BOSSS_solution(size, N, prob)

print(sol)
plot(sol)
```

```{r}
for(i in 1:10) {
  sol <- iterate(sol, prob, N) 
}
print(sol)
plot(sol)
```

## Simple two-sample t-test

To do.

```{r}
sim_trial <- function(design, hypothesis){
  # Exctract the design variables and parameters
  design <- as.numeric(design); hypothesis <- as.numeric(hypothesis)
  n <- design[1]; k <- design[2]
  mu <- hypothesis[1]; var_u <- hypothesis[2]; var_e <- hypothesis[3]
  
  # Number of patients in each cluster
  m <- n/k
  # SD of cluster means
  s_c <- sqrt(var_u + var_e/m)
  # Simulate cluster means
  x0 <- stats::rnorm(k, 0, s_c); x1 <- stats::rnorm(k, mu, s_c)
  
  return(c(s = stats::t.test(x0, x1)$p.value >= 0.05, n = n, k = k))
}

det_obj <- function(design){
    #o <- matrix(design, ncol = 2)[,1:2]
    #c(s = NA, p = o[1], c = o[2])
  cbind(NA, design)
}

design_space <- data.frame(name = c("n", "k"),
                 low = c(50, 5),
                 up = c(500, 50),
                 int = c(TRUE, TRUE))

hypotheses <- data.frame(mu = c(0.3, 0),
                   var_u = c(0.05, 0.05),
                   var_e = c(0.95, 0.95))

constraints <- data.frame(name = "a",
                   out_i = 1,
                   hyp_i = 2,
                   nom = 0.2,
                   delta = 0.975,
                   stoch = TRUE)

objectives <- data.frame(name = c("f1", "f2", "f3"),
                 out_i = c(2, 3, 1),
                 hyp_i = c(1, 1, 1),
                 weight = c(2, 5, 50),
                 stoch = c(FALSE, FALSE, TRUE))
```

## Hybrid Bayesian multilevel model

Consider the design of a cluster randomised trial with $m$ clusters per arm, $n$ patients per cluster and a binary outcome for each patient. The trial will be analysed using a mixed effects model fitted using the `glmer` function from the `lme4` package, using a Wald test of the null hypothesis of no difference between arms at the 0.025 (one-sided) level.

A design is defined by the choice of $m$ and $n$. We want to find designs which minimise the number of clusters, the number of patients, and the overal probability of failure when marginalising over a joint prior distribution on the model parameters. A hypothesis, then, is not a set of point parameter values but a set of prior hyperparameter values.

In contrast to the NIFTy example, we now have some objectives (the number of clusters and number of patients) which are deterministic. To acknowledge this we specify two functions: the simulation function is as before, returning a single Monte Carlo sample of the stochastic outputs. The deterministic function takes the same arguments (i.e. a design and a hypothesis), and returns the deterministic outputs (as with the simulation function, this should take the form of a named vector).

```{r}
library(lme4)

sim_bayes <- function(design = c(20, 10),
                      hypothesis = c(0.2, 15, 0.3, 15, 100, 10)) {
  
  design <- as.numeric(design); hypothesis <- as.numeric(hypothesis)
  
  # Number of clusters in each arm
  m <- round(design[1])
  # Number of patients per cluster
  n <- round(design[2])
  
  # Overall probability of outcome and effective prior sample size
  # in each arm
  theta_0 <- hypothesis[1]; eff_n_0 <- hypothesis[2]
  theta_1 <- hypothesis[3]; eff_n_1 <- hypothesis[4]
  
  # het_n is the effective sample size used in the beta prior
  # of cluster rates - it encapsulates our degree of certainty
  # about the between-cluster variance.
  het_n_mean <- hypothesis[5]; het_n_sd <- hypothesis[6]
  
  het_n <- rgamma(1, shape = (het_n_mean/het_n_sd)^2, rate = het_n_mean/het_n_sd^2) + 2
  
  a_0 <- theta_0*eff_n_0; b_0 <- eff_n_0 - a_0
  a_1 <- theta_1*eff_n_1; b_1 <- eff_n_1 - a_1
  
  # Simulate overall rate in control arm
  r_0 <- rbeta(1, a_0, b_0)
  # Simulate cluster rates in control arm
  c_r_0 <- rbeta(m, r_0*het_n, het_n - r_0*het_n)
  # Simulate patient outcomes in control arm
  p_0 <- rbinom(m*n, 1, rep(c_r_0, each = n))
  
    # Simulate overall rate in exp arm
  r_1 <- rbeta(1, a_1, b_1)
  # Simulate cluster rates in exp arm
  c_r_1 <- rbeta(m, r_1*het_n, het_n - r_1*het_n)
  # Simulate patient outcomes in exp  arm
  p_1 <- rbinom(m*n, 1, rep(c_r_1, each = n))
  
  # Create data frame
  df <- data.frame(trt = rep(c(0, 1), each = n*m),
                   c_id = rep(1:(2*m), each = n),
                   y = c(p_0, p_1))
  
  fit <- suppressMessages(glmer(y ~ trt + (1 | c_id), family = binomial, data = df))
  
  est <- fixef(fit)[[2]]
  se <- sqrt(diag(vcov(fit)))[[2]]
  
  s <- est/se < qnorm(0.975)
  return(c(s = s))
}

sim_bayes()

det_bayes <- function(design = c(20, 10),
                      hypothesis = c(0.2, 15, 0.3, 15, 100, 10))
{
  design <- as.numeric(design); hypothesis <- as.numeric(hypothesis)
  return(c(m = design[1], N = 2*design[1]*design[2]))
}

det_bayes()
```



```{r}
design_space <- design_space(name = c("m", "n"),
                             lower = c(10,5), 
                             upper = c(50,40))

hypotheses <- hypotheses(par_name = c("theta_0", "eff_n_0", "theta_1", "eff_n_1", "het_n_mean", "het_n_sd"),
                         values = matrix(c(0.2, 15, 0.3, 15, 100, 10), ncol = 1),
                         hyp_names = c("alt"))

constraints <- constraints(name = c("a"),
                   out = c("s"),
                   hyp = c("alt"),
                   nom = c(0.4),
                   delta =c(0.95),
                   stoch = c(TRUE))

objectives <- objectives(name = c("f1", "f2", "f3"),
                 out = c("s", "m", "N"),
                 hyp = c("alt", "alt", "alt"),
                 weight = c(100, 10, 1),
                 stoch = c(TRUE, FALSE, FALSE))

prob <- BOSSS_problem(sim_bayes, design_space, hypotheses, constraints, objectives, det_func = det_bayes)

size <- 20
N <- 100

sol <- BOSSS_solution(size, N, prob)

print(sol)
plot(sol)
```

```{r}
for(i in 1:1) {
  sol <- iterate(sol, prob, N) 
}
print(sol)
plot(sol)
```

```{r}
sa_hypothesis <- data.frame(hyp = "alt",
                            sa_param = 5,
                            lower = 10,
                            upper = 100)

sa <- as.data.frame(sensitivity(sa_hypothesis, design, prob, sol, num_eval = 5, N = 100))

# Plot these to examine sensitivity:
ggplot(sa, aes(p01relative, s_m)) + geom_point() + 
  geom_errorbar(aes(ymin = s_m - 1.96*sqrt(s_v), ymax = s_m + 1.96*sqrt(s_v))) +
  theme_minimal()

ggplot(sa, aes(p01relative, n_m)) + geom_point() + 
  geom_errorbar(aes(ymin = n_m - 1.96*sqrt(n_v), ymax = n_m + 1.96*sqrt(n_v))) +
  theme_minimal()
```


## Multi-state models

## Neil's example

```{r}
source("C:/Users/meddwilb/OneDrive - University of Leeds/Documents/Research/Projects/BOSSS example/PowCalc_Binary_DGM_5_freq.R")
source("C:/Users/meddwilb/OneDrive - University of Leeds/Documents/Research/Projects/BOSSS example/PowCalc_Binary_Analysis_5_freq.R")
source("C:/Users/meddwilb/OneDrive - University of Leeds/Documents/Research/Projects/BOSSS example/PowCalc_Binary_DGM_5a_freq.R")
source("C:/Users/meddwilb/OneDrive - University of Leeds/Documents/Research/Projects/BOSSS example/PowCalc_Binary_BPPcalculator_5_freq.R")
source("C:/Users/meddwilb/OneDrive - University of Leeds/Documents/Research/Projects/BOSSS example/PowCalc_Binary_Design_5_freq.R")

# n = Total sample size
# n_interim = Interim sample size
# p0_true and p1_true:= the assumed true probabilities of an event in the control (0) and experimental (1) arms
# alpha = the threshold used at the final analysis to determine whether the result is significant or not. At final analysis, the bounds of the 100*(1 - alpha/2)% credible interval of the log-odds ratio is compared to 0.
# BPP_numsims:= how many simulations are used to evaluate Bayesian Predictive Power at the interim analysis
# BPP_threshold:= BPP must be >= this value in order for the trial to continue to final analysis
# Remaining: the analysis of the trial assumes beta prior distributions on p0 and p1. p0 ~ beta(APr_p0_alpha, APr_p0_beta) and p1 ~ beta(APr_p1_alpha, APr_p1_beta)
# by default these are both set to be beta(1,1), non-informative.

sim_trial <- function(design = c(400, 200, 0.2, 0.05), hypothesis = c(0.2, 0.1)) {
  
  design <- as.numeric(design); hypothesis <- as.numeric(hypothesis)
  
  n <- design[1]; n_interim <- design[2]; BPP_threshold <- design[3]; alpha <- design[4]
  p0_true <- hypothesis[1]; p1_true <- hypothesis[2]
  
  BPP_numsims=200; APr_p0_alpha=1; APr_p0_beta=1; APr_p1_alpha=1; APr_p1_beta=1
  
  # Generate a dataset
  chk_DGM_output <- DGM_run(n, n_interim, p0_true, p1_true)
  #Debugging
  # print("DGM output DONE")
  
  # Analyse
  chk_Analysis_output <- Analysis_run(compiled_model, interim_data=chk_DGM_output$interim_data, chk_DGM_output$full_data, alpha, BPP_numsims, APr_p0_alpha, APr_p0_beta, APr_p1_alpha, APr_p1_beta)
  #Debugging
  # print("Analysis output DONE")
  BPP_sig <- (chk_Analysis_output$BPP >= BPP_threshold)
  final_sig <- (chk_Analysis_output$beta1quants[2] <= 0 | chk_Analysis_output$beta1quants[1] >= 0)
  trial_sig <- (BPP_sig & final_sig)
  trial_non_sig <- !trial_sig
  N_trial <- n_interim + BPP_sig*(n-n_interim)
  
  # Return the results
  results <- c(
    # BPP_sig = BPP_sig,
    trial_sig = trial_sig,
    trial_non_sig = trial_non_sig,
    N_trial = N_trial
  )
  
  # Return the results
  # results <- c(
  #   beta1_est_int = chk_Analysis_output$beta1mean_interim,
  #   beta1_lower_int = chk_Analysis_output$beta1quants_interim[1],
  #   beta1_upper_int = chk_Analysis_output$beta1quants_interim[2],
  #   beta0_est_int = chk_Analysis_output$beta0mean_interim,
  #   BPP = chk_Analysis_output$BPP,
  #   BPP_sig = BPP_sig,
  #   beta1_est_final = chk_Analysis_output$beta1mean,
  #   beta1_lower_final = chk_Analysis_output$beta1quants[1],
  #   beta1_upper_final = chk_Analysis_output$beta1quants[2],
  #   beta0_est_final = chk_Analysis_output$beta0mean,
  #   final_sig = final_sig,
  #   trial_sig = trial_sig,
  #   trial_non_sig = trial_non_sig,
  #   N_trial = N_trial
  # )
  
  return(results)
}  

# For example,
sim_trial()
```


```{r}
design_space <- design_space(name = c("n", "n_interim", "BPP_threshold", "alpha"),
                             lower = c(700,300,0,0), 
                             upper = c(1400,650,1,0.2))
```


```{r}
hypotheses <- hypotheses(par_name = c("p0_true", "p1_true"),
                         values = matrix(c(0.12, 0.12,
                                           0.12, 0.06), ncol = 2),
                         hyp_names = c("null", "alt"))
```


```{r}
constraints <- constraints(name = c("a", "b"),
                   out = c("trial_sig", "trial_non_sig"),
                   hyp = c("null", "alt"),
                   nom = c(0.2, 0.4),
                   delta =c(0.95, 0.95),
                   stoch = c(TRUE, TRUE))
```


```{r}
objectives <- objectives(name = c("f1", "f2", "f3"),
                 out = c("trial_sig", "trial_non_sig", "N_trial"),
                 hyp = c("null", "alt", "null"),
                 weight = c(100, 100, 1),
                 stoch = c(TRUE, TRUE, TRUE),
                 binary = c(TRUE, TRUE, FALSE))
```

We now put this simulation function and set of data frames together to create an object of class `BOSSS_problem`. 

```{r}
prob <- BOSSS_problem(sim_trial, design_space, hypotheses, constraints, objectives)
```

### Initialisation


```{r}
size <- 40
N <- 50

sol <- BOSSS_solution(size, N, prob)

print(sol)
plot(sol)
```


### Iteration

```{r}
for(i in 1:10) {
  sol <- iterate(sol, prob, N) 
}
print(sol)
plot(sol)
```


### Diagnostics

```{r}
# Pick a specific design from the Pareto set
design <- sol$p_set[1,]

plots <- one_d_plots(design, prob, sol)

for(i in 1:length(plots)) {
  print(plots[[i]])
}
```

```{r}
r <- check_point(design, prob, sol, N=50) 
```

```{r}
r <- check_point(design, prob, sol, N=50, r) 
```


